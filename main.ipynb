{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temat 1. Własna sieć neuronowa\n",
    "Celem projektu jest samodzielne zaimplementowanie prostej sieci neuronowej i przetestowanie jej na wybranym przez siebie przykładzie.\n",
    "\n",
    "Podstawowym założeniem projektu jest własnoręczne zaimplementowanie automatycznego różniczkowania w celu wyznaczania pochodnych warstw potrzebnych do wstecznej   propagacji błędów. Dopuszczalne jest wykorzystanie różniczkowanie w przód (ang. Forward Accumulation), różniczkowanie w tył (ang. Reverse Accumulation) oraz różniczkowanie oparte o generację kodu (ang. source-to-source differentiation).\n",
    "\n",
    "Do implementacji optymalizatorów dokonujących właściwego uczenia sieci można wykorzystać kod zamieszczony w książce \"Algorithms for Optimization\" [1].\n",
    "\n",
    "klasyfikacja cyfr (Digits MNIST dataset)  \n",
    "Bibliografia:  \n",
    "[1] Mykel J. Kochenderfer, Tim A. Wheeler, 2019, Algorithms for Optimization, MIT Press.  \n",
    "[2] Martin T. Hagan et. al, Neural Network Design, pp. 915-918, url: https://hagan.okstate.edu/NNDesign.pdf  \n",
    "[3] 3blue1brown: czym są sieci neuronowe? https://www.youtube.com/watch?v=aircAruvnKk  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `e:\\Documents\\1. Studia\\MAGISTERSKIE\\Algorytmy w inż danych\\Custom-Julia-NN\\project`\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\"./project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `E:\\Documents\\1. Studia\\MAGISTERSKIE\\Algorytmy w inż danych\\Custom-Julia-NN\\project\\Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `E:\\Documents\\1. Studia\\MAGISTERSKIE\\Algorytmy w inż danych\\Custom-Julia-NN\\project\\Manifest.toml`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `E:\\Documents\\1. Studia\\MAGISTERSKIE\\Algorytmy w inż danych\\Custom-Julia-NN\\project\\Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "to `E:\\Documents\\1. Studia\\MAGISTERSKIE\\Algorytmy w inż danych\\Custom-Julia-NN\\project\\Manifest.toml`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `E:\\Documents\\1. Studia\\MAGISTERSKIE\\Algorytmy w inż danych\\Custom-Julia-NN\\project\\Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `E:\\Documents\\1. Studia\\MAGISTERSKIE\\Algorytmy w inż danych\\Custom-Julia-NN\\project\\Manifest.toml`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `E:\\Documents\\1. Studia\\MAGISTERSKIE\\Algorytmy w inż danych\\Custom-Julia-NN\\project\\Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `E:\\Documents\\1. Studia\\MAGISTERSKIE\\Algorytmy w inż danych\\Custom-Julia-NN\\project\\Manifest.toml`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `E:\\Documents\\1. Studia\\MAGISTERSKIE\\Algorytmy w inż danych\\Custom-Julia-NN\\project\\Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `E:\\Documents\\1. Studia\\MAGISTERSKIE\\Algorytmy w inż danych\\Custom-Julia-NN\\project\\Manifest.toml`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Pkg.add(\"MLDatasets\")\n",
    "Pkg.add(\"IJulia\")\n",
    "Pkg.add(\"StableRNGs\")\n",
    "Pkg.add(\"DataFrames\")\n",
    "Pkg.add(\"PyPlot\")\n",
    "ENV[\"PYTHON\"]=\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train X shape: (28, 28, 1, 60000)\n",
      "Test X shape: (28, 28, 1, 10000)\n",
      "Train Y shape: (10, 60000)\n",
      "Test Y shape: (10, 10000)\n"
     ]
    }
   ],
   "source": [
    "using MLDatasets\n",
    "using StableRNGs\n",
    "train_x, train_y = MNIST.traindata(Float32);\n",
    "test_x, test_y = MNIST.testdata(Float32);\n",
    "Y_labels = [0,1,2,3,4,5,6,7,8,9]\n",
    "train_size = size(train_x)\n",
    "test_size = size(test_x)\n",
    "train_x = reshape(train_x, (train_size[1],train_size[2],1,train_size[3]))\n",
    "test_x = reshape(test_x, (test_size[1],test_size[2],1,test_size[3]))\n",
    "println(\"Train X shape: \",size(train_x));\n",
    "println(\"Test X shape: \",size((test_x)));\n",
    "test_test_y = test_y\n",
    "test_y= Y_labels  .== permutedims(test_y)\n",
    "train_y= Y_labels .== permutedims(train_y)\n",
    "println(\"Train Y shape: \",size(train_y));\n",
    "println(\"Test Y shape: \",size((test_y)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Automatic differentation- foward accumulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dual number\n",
    "struct Dual{T <:Number} <:Number\n",
    "     v::T\n",
    "    dv::T\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overloading functions and operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "log (generic function with 24 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import Base: +, -, *, /\n",
    "-(x::Dual)          = Dual(-x.v,       -x.dv)\n",
    "+(x::Dual, y::Dual) = Dual( x.v + y.v,  x.dv + y.dv)\n",
    "-(x::Dual, y::Dual) = Dual( x.v - y.v,  x.dv - y.dv)\n",
    "*(x::Dual, y::Dual) = Dual( x.v * y.v,  x.dv * y.v + x.v * y.dv)\n",
    "/(x::Dual, y::Dual) = Dual( x.v / y.v, (x.dv * y.v - x.v * y.dv)/y.v^2)\n",
    "\n",
    "import Base: abs, sin, cos, tan, exp, sqrt, isless, log, max, min\n",
    "abs(x::Dual)  = Dual(abs(x.v),sign(x.v)*x.dv)\n",
    "sin(x::Dual)  = Dual(sin(x.v), cos(x.v)*x.dv)\n",
    "cos(x::Dual)  = Dual(cos(x.v),-sin(x.v)*x.dv)\n",
    "tan(x::Dual)  = Dual(tan(x.v), one(x.v)*x.dv + tan(x.v)^2*x.dv)\n",
    "exp(x::Dual)  = Dual(exp(x.v), exp(x.v)*x.dv)\n",
    "sqrt(x::Dual) = Dual(sqrt(x.v),.5/sqrt(x.v) * x.dv)\n",
    "isless(x::Dual, y::Dual) = x.v < y.v;\n",
    "max(x::Dual, y::Dual) = max(x.v , y.v); # what aboud dv?\n",
    "min(x::Dual, y::Dual) = min(x.v , y.v);\n",
    "log(x::Dual) = Dual(log(x.v), (1/abs(x.v))*x.dv) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dual{Float64}[Dual(1, 2), Dual(3, 0)] = Dual{Float64}[(1.0) + [2.0ϵ], (3.0) + [0.0ϵ]]\n",
      "Dual{Float64}[1, 2, 3] = Dual{Float64}[(1.0) + [0.0ϵ], (2.0) + [0.0ϵ], (3.0) + [0.0ϵ]]\n",
      "Dual(1, 2) * 3 = (3) + [6ϵ]\n"
     ]
    }
   ],
   "source": [
    "import Base: convert, promote_rule\n",
    "\n",
    "convert(::Type{Dual{T}}, x::Dual) where T = Dual(convert(T, x.v), convert(T, x.dv))\n",
    "@show Dual{Float64}[Dual(1,2), Dual(3,0)];\n",
    "convert(::Type{Dual{T}}, x::Number) where T = Dual(convert(T, x), zero(T))\n",
    "@show Dual{Float64}[1, 2, 3];\n",
    "promote_rule(::Type{Dual{T}}, ::Type{R}) where {T,R} = Dual{promote_type(T,R)}\n",
    "@show Dual(1,2) * 3;\n",
    "\n",
    "import Base: show\n",
    "show(io::IO, x::Dual) = print(io, \"(\", x.v, \") + [\", x.dv, \"ϵ]\");\n",
    "value(x::Dual) = x.v;\n",
    "partials(x::Dual) = x.dv;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "derivative (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "D = derivative(f, x) = partials(f(Dual(x, one(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "jacobian (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "J = function jacobian(f, args::Vector{T}) where {T <:Number}\n",
    "    jacobian_columns = Matrix{T}[]\n",
    "    \n",
    "    for i=1:length(args)\n",
    "        x = Dual{T}[]\n",
    "        for j=1:length(args)\n",
    "            seed = (i == j)\n",
    "            push!(x, seed ?\n",
    "                Dual(args[j], one(args[j])) :\n",
    "                Dual(args[j],zero(args[j])) )\n",
    "        end\n",
    "        column = partials.([f(x)...])\n",
    "        push!(jacobian_columns, column[:,:])\n",
    "    end\n",
    "    hcat(jacobian_columns...)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hessian (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "H = function hessian(f, args::Vector)\n",
    "    ∇f(x::Vector) = J(f, x)\n",
    "    J(∇f, args)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "diagonal (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import LinearAlgebra: diagm\n",
    "diagonal(m) = diagm(0 => vec(m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tanh (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "softmax(x)  =  exp.(x) ./ sum(exp.(x));\n",
    "dsoftmax(x) = (softmax(x) |> diagonal) .- softmax(x) * (softmax(x) |> transpose);\n",
    "\n",
    "ReLU(x) = max(zero(x), x)\n",
    "identityFunction(x) = x\n",
    "\n",
    "σ(x) = one(x) / (one(x) + exp(-x))\n",
    "tanh(x) = 2.0 / (one(x) + exp(-2.0x)) - one(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "binary_cross_entropy (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean_squared_loss(y::Vector, ŷ::Vector) = sum(0.5(y - ŷ).^2)\n",
    "\n",
    "function binary_cross_entropy(y::Vector, ŷ::Vector) \n",
    "    epsilon = eps(1.0)\n",
    "    ## Avoding 0 , 1 in log argument\n",
    "    ŷ = [max(i, epsilon) for i in ŷ]\n",
    "    ŷ = [min(i, 1-epsilon) for i in ŷ]\n",
    "    return -sum(y .* log.(ŷ) + (1 .- y) .* log.(1 .- ŷ)) / length(y)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_cross_entropy([1, 1, 0.01], [1, 1, 0]) = 0.1201455112970574\n",
      "mean_squared_loss([1, 1, 1], [0, 0, 0]) = 1.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@show binary_cross_entropy([1,1,0.01],[1,1,0])\n",
    "@show mean_squared_loss([1,1,1],[0,0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Net layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fullyconnected (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fullyconnected(w::Vector, n::Number, m::Number, v::Vector, activation::Function) = activation.(reshape(w, n, m) * v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "net (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function net(x, wh, wo, y)\n",
    "    x̂ = fullyconnected(wh, 10, 2, x, σ)\n",
    "    ŷ = fullyconnected(wo, 1, 10, x̂, u->u)\n",
    "    E = mean_squared_loss(y, ŷ)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "net (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "function net(x, wh, wo, y)\n",
    "    x̂ = fullyconnected(wh, 16, 24, x, σ)\n",
    "    ŷ = fullyconnected(wo, 9, 16, x̂, u -> u)\n",
    "    E = mean_squared_loss(y, ŷ)\n",
    "    return E\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "foward (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function foward(x, wh, wo, y)\n",
    "    x̂ = fullyconnected(wh, 16, 24, x, σ)\n",
    "    ŷ = fullyconnected(wo, 9, 16, x̂, u -> u)\n",
    "    return ŷ\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Layer\n",
    "    Wh::Matrix\n",
    "    dWb::Matrix\n",
    "    activation::Function\n",
    "    Layer(in::Int64,out::Int64,activation::Function=identityFunction) = new(randn(in,out),randn(in,out),activation) #constructor\n",
    "end\n",
    "\n",
    "struct Network\n",
    "    layers::Vector{Layer} \n",
    "    Network(layers::Vararg{Layer}) = new(vcat(layers...)) \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x  = randn(23,3)\n",
    "length(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "show (generic function with 352 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import Base: show, summary\n",
    "show(io::IO, x::Network) = begin\n",
    "    print(\"Neural Netowrk | \") \n",
    "    print(\"Number of layers : $(length(x.layers))\") \n",
    "    println(\"\\nLayers: \")\n",
    "    for (i,layer) in enumerate(x.layers)\n",
    "        print(\"\\t$i. \")\n",
    "        print(layer)\n",
    "    end\n",
    "end\n",
    "show(io::IO, x::Layer) = begin\n",
    "    print(\"Layer : \") \n",
    "    print(\"shape: $(size(x.Wh))\") \n",
    "    println(\" - activation function: $(x.activation)\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "(m::Layer)(x) = m.activation.(m.Wh*x) \n",
    "(m::Network)(x) = reduce((left,right)->right∘left,m.layers)(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.38702016743074713, 0.9999999999753668, 0.7586174903548171, 0.05061267343381272, 0.9999999995080562, 1.0, 0.9520045105509686, 0.9996748555675552, 0.9999543275093726, 0.26779068272377854, 1.010596345649264e-12, 6.078420135883192e-6, 0.17654442783098853, 4.020030163641174e-7, 7.843476088959065e-8, 1.1366028531695391e-7, 3.8058345309966866e-5, 4.528561594046657e-16, 0.9999999992811586, 0.9513084624673384, 7.087204648294812e-11, 3.1572658150188503e-6, 3.4512707155579907e-12, 1.2920850132141002e-33]\n",
      "[0.6940371047105774, 0.18591858973217346, 0.3452496961204213, 0.9898106957684135, 0.4815791392207472, 0.35062784296583605, 0.3729617183755609, 0.8801727792813636, 0.47626472647087414]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.6823716800690462"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = [10, 4, 4, 8, 4, 1, 9, 0, 0, 1, 10, 3, 5, 3, 5, 4, 6, 5, 10, 6, 2, 10, 8, 1]   \n",
    "y = [0,0,0,0,0,0,1,0,0]\n",
    "\n",
    "twoLayerNeuralNet = Network(Layer(24,24,σ),Layer(9,24,σ)) #instantiate a two-layer network\n",
    "for (i, layer) in enumerate(twoLayerNeuralNet.layers)\n",
    "    x=layer(x)\n",
    "    println(x)\n",
    "end\n",
    "L = mean_squared_loss(y, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function train(epochs::Int, X_data, y_data)\n",
    "    X_data = X_data\n",
    "    y_data = y_data\n",
    "\n",
    "    Wh = randn(16, 24)\n",
    "    Wo = randn(9, 16)\n",
    "    dWh = similar(Wh)\n",
    "    dWo = similar(Wo)\n",
    "    \n",
    "    @assert size(X_data, 1) == size(y_data, 1)\n",
    "\n",
    "    Loss_history = Float64[]\n",
    "\n",
    "    for epoch = 1:epochs\n",
    "\n",
    "        for (i, x) in enumerate(X_data)\n",
    "\n",
    "            y = y_data[i]\n",
    "            ŷ = foward(x, Wh[:], Wo[:], y[i])\n",
    "            L = mean_squared_loss(y, ŷ)\n",
    "        \n",
    "            dnet_Wh(x, wh, wo, y) = J(w -> net(x, w, wo, y), wh)\n",
    "            dWh[:] = dnet_Wh(x, Wh[:], Wo[:], y)\n",
    "            dnet_Wo(x, wh, wo, y) = J(w -> net(x, wh, w, y), wo)\n",
    "            dWo[:] = dnet_Wo(x, Wh[:], Wo[:], y)\n",
    "        \n",
    "            push!(Loss_history, L)\n",
    "\n",
    "            Wh -= 0.1dWh\n",
    "            Wo -= 0.1dWo\n",
    "        end\n",
    "    end\n",
    "    return Loss_history\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16.540138471298917, 11.494876470909247, 3.3007083511711306, 1.4424595384434693, 0.7701906150582981, 0.5083567188577196, 0.5281771100843748, 0.5066547888446595]"
     ]
    }
   ],
   "source": [
    "data = [[10, 4, 4, 8, 4, 1, 9, 0, 0, 1, 10, 3, 5, 3, 5, 4, 6, 5, 10, 6, 2, 10, 8, 1],\n",
    "        [9, 2, 4, 7, 8, 5, 8, 1, 5, 4, 5, 8, 7, 5, 9, 9, 1, 9, 5, 10, 10, 0, 3, 3]]\n",
    "y = [[0,0,0,0,0,0,1,0,0],[0,1,0,0,0,0,0,0,0]]\n",
    "history=train(4,data,y)\n",
    "print(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
